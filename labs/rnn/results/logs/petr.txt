2020-12-23 22:44:51.872027: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-12-23 22:44:51.872692: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-23 22:44:52.691554: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/30
2833/2833 [==============================] - 234s 80ms/step - loss: 3.0442

Epoch 00001: loss improved from inf to 3.00514, saving model to results/petr/epoch_01__loss_3.0051.hdf5
Epoch 2/30
2833/2833 [==============================] - 225s 79ms/step - loss: 2.9596

Epoch 00002: loss improved from 3.00514 to 2.94000, saving model to results/petr/epoch_02__loss_2.9400.hdf5
Epoch 3/30
2833/2833 [==============================] - 248s 88ms/step - loss: 2.8858

Epoch 00003: loss improved from 2.94000 to 2.87288, saving model to results/petr/epoch_03__loss_2.8729.hdf5
Epoch 4/30
2833/2833 [==============================] - 255s 90ms/step - loss: 2.8407

Epoch 00004: loss improved from 2.87288 to 2.83445, saving model to results/petr/epoch_04__loss_2.8344.hdf5
Epoch 5/30
2833/2833 [==============================] - 260s 92ms/step - loss: 2.8121

Epoch 00005: loss improved from 2.83445 to 2.80076, saving model to results/petr/epoch_05__loss_2.8008.hdf5
Epoch 6/30
2833/2833 [==============================] - 265s 94ms/step - loss: 2.7774

Epoch 00006: loss improved from 2.80076 to 2.76746, saving model to results/petr/epoch_06__loss_2.7675.hdf5
Epoch 7/30
2833/2833 [==============================] - 270s 95ms/step - loss: 2.7366

Epoch 00007: loss improved from 2.76746 to 2.73155, saving model to results/petr/epoch_07__loss_2.7316.hdf5
Epoch 8/30
2833/2833 [==============================] - 266s 94ms/step - loss: 2.7109

Epoch 00008: loss improved from 2.73155 to 2.69609, saving model to results/petr/epoch_08__loss_2.6961.hdf5
Epoch 9/30
2833/2833 [==============================] - 272s 96ms/step - loss: 2.6639

Epoch 00009: loss improved from 2.69609 to 2.65858, saving model to results/petr/epoch_09__loss_2.6586.hdf5
Epoch 10/30
2833/2833 [==============================] - 281s 99ms/step - loss: 2.6308

Epoch 00010: loss improved from 2.65858 to 2.61945, saving model to results/petr/epoch_10__loss_2.6195.hdf5
Epoch 11/30
2833/2833 [==============================] - 288s 102ms/step - loss: 2.5862

Epoch 00011: loss improved from 2.61945 to 2.57565, saving model to results/petr/epoch_11__loss_2.5757.hdf5
Epoch 12/30
2833/2833 [==============================] - 295s 104ms/step - loss: 2.5431

Epoch 00012: loss improved from 2.57565 to 2.53541, saving model to results/petr/epoch_12__loss_2.5354.hdf5
Epoch 13/30
2833/2833 [==============================] - 296s 105ms/step - loss: 2.4946

Epoch 00013: loss improved from 2.53541 to 2.49334, saving model to results/petr/epoch_13__loss_2.4933.hdf5
Epoch 14/30
2833/2833 [==============================] - 282s 100ms/step - loss: 2.4504

Epoch 00014: loss improved from 2.49334 to 2.45384, saving model to results/petr/epoch_14__loss_2.4538.hdf5
Epoch 15/30
2833/2833 [==============================] - 286s 101ms/step - loss: 2.4191

Epoch 00015: loss improved from 2.45384 to 2.41932, saving model to results/petr/epoch_15__loss_2.4193.hdf5
Epoch 16/30
2833/2833 [==============================] - 292s 103ms/step - loss: 2.3858

Epoch 00016: loss improved from 2.41932 to 2.38216, saving model to results/petr/epoch_16__loss_2.3822.hdf5
Epoch 17/30
2833/2833 [==============================] - 286s 101ms/step - loss: 2.3544

Epoch 00017: loss improved from 2.38216 to 2.34845, saving model to results/petr/epoch_17__loss_2.3484.hdf5
Epoch 18/30
2833/2833 [==============================] - 292s 103ms/step - loss: 2.3094

Epoch 00018: loss improved from 2.34845 to 2.31333, saving model to results/petr/epoch_18__loss_2.3133.hdf5
Epoch 19/30
2833/2833 [==============================] - 294s 104ms/step - loss: 2.2815

Epoch 00019: loss improved from 2.31333 to 2.28363, saving model to results/petr/epoch_19__loss_2.2836.hdf5
Epoch 20/30
2833/2833 [==============================] - 299s 106ms/step - loss: 2.2492

Epoch 00020: loss improved from 2.28363 to 2.25657, saving model to results/petr/epoch_20__loss_2.2566.hdf5
Epoch 21/30
2833/2833 [==============================] - 286s 101ms/step - loss: 2.2159

Epoch 00021: loss improved from 2.25657 to 2.22657, saving model to results/petr/epoch_21__loss_2.2266.hdf5
Epoch 22/30
2833/2833 [==============================] - 258s 91ms/step - loss: 2.1917

Epoch 00022: loss improved from 2.22657 to 2.19817, saving model to results/petr/epoch_22__loss_2.1982.hdf5
Epoch 23/30
2833/2833 [==============================] - 195s 69ms/step - loss: 2.1668

Epoch 00023: loss improved from 2.19817 to 2.17439, saving model to results/petr/epoch_23__loss_2.1744.hdf5
Epoch 24/30
2833/2833 [==============================] - 184s 65ms/step - loss: 2.1319

Epoch 00024: loss improved from 2.17439 to 2.15012, saving model to results/petr/epoch_24__loss_2.1501.hdf5
Epoch 25/30
2833/2833 [==============================] - 192s 68ms/step - loss: 2.1168

Epoch 00025: loss improved from 2.15012 to 2.12738, saving model to results/petr/epoch_25__loss_2.1274.hdf5
Epoch 26/30
2833/2833 [==============================] - 191s 68ms/step - loss: 2.0955

Epoch 00026: loss improved from 2.12738 to 2.11122, saving model to results/petr/epoch_26__loss_2.1112.hdf5
Epoch 27/30
2833/2833 [==============================] - 183s 65ms/step - loss: 2.0802

Epoch 00027: loss improved from 2.11122 to 2.09030, saving model to results/petr/epoch_27__loss_2.0903.hdf5
Epoch 28/30
2833/2833 [==============================] - 186s 66ms/step - loss: 2.0577

Epoch 00028: loss improved from 2.09030 to 2.06948, saving model to results/petr/epoch_28__loss_2.0695.hdf5
Epoch 29/30
2833/2833 [==============================] - 209s 74ms/step - loss: 2.0379

Epoch 00029: loss improved from 2.06948 to 2.05070, saving model to results/petr/epoch_29__loss_2.0507.hdf5
Epoch 30/30
2833/2833 [==============================] - 165s 58ms/step - loss: 2.0247

Epoch 00030: loss improved from 2.05070 to 2.03722, saving model to results/petr/epoch_30__loss_2.0372.hdf5